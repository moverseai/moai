# @package model.parameters.optimizers.adamw

_target_: torch.optim.AdamW
lr: 0.0001
betas: [0.9, 0.999]
eps: 1e-8
weight_decay: 0.01
amsgrad: false
maximize: false
foreach: null
capturable: false
differentiable: false
fused: null